{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os as os\n",
    "import nltk as nltk\n",
    "import numpy as np\n",
    "import re\n",
    "import requests\n",
    "from sklearn.datasets import load_svmlight_files\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootDirectory = \"aclImdb/\"\n",
    "trainPos = \"train/pos/\"\n",
    "trainNeg = \"train/neg/\"\n",
    "testPos = \"test/pos/\"\n",
    "testNeg = \"test/neg/\"\n",
    "def readDataFromFile(path):\n",
    "    rList = np.array([], dtype='object')\n",
    "    for file in os.listdir(path):\n",
    "        rList = np.append(rList, open(path + file, encoding=\"utf8\").read()) \n",
    "    return rList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the data set for problem 1 and 2\n",
    "positiveTrainReviwList = readDataFromFile(rootDirectory + trainPos)\n",
    "negativeTrainReviwList = readDataFromFile(rootDirectory + trainNeg)\n",
    "positiveTestReviwList = readDataFromFile(rootDirectory + testPos)\n",
    "negativeTestReviwList = readDataFromFile(rootDirectory + testNeg)\n",
    "xTrain = np.concatenate((positiveTrainReviwList, negativeTrainReviwList), axis = None)\n",
    "yTrain = np.concatenate((np.ones(len(positiveTrainReviwList)), np.zeros(len(negativeTrainReviwList))), axis = None)\n",
    "xTest = np.concatenate((positiveTestReviwList, negativeTestReviwList), axis = None)\n",
    "yTest = np.concatenate((np.ones(len(positiveTestReviwList)), np.zeros(len(negativeTestReviwList))), axis = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given the level of testing data and level of predicted data, this method will return true positive, \n",
    "# true negative, false positive and false negative\n",
    "def calcaulateConfusionMatrix(testData, predictedData):\n",
    "    i = 0\n",
    "    tp = fp = tn = fn = 0\n",
    "    for p in  predictedData:\n",
    "        if p == 1:\n",
    "            if p == testData[i]:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fp += 1\n",
    "        else:\n",
    "            if p == testData[i]:\n",
    "                tn += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        i += 1\n",
    "    return tp, tn, fp, fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given the level of testing data and level of predicted data, this will calcualte Precision using confusion matrix\n",
    "def calculatePrecision(yTrue, yPredicted):\n",
    "    tp, tn, fp, fn = calcaulateConfusionMatrix(yTrue, yPredicted)\n",
    "    return tp / (tp + fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given the level of testing data and level of predicted data, this will calcualte Recall using confusion matrix\n",
    "def calculateRecall(yTrue, yPredicted):\n",
    "    tp, tn, fp, fn = calcaulateConfusionMatrix(yTrue, yPredicted)\n",
    "    return tp / (tp + fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given the level of testing data and level of predicted data, this will calcualte fScore using confusion matrix\n",
    "def calculateFscore(yTrue, yPredicted):\n",
    "    tp, tn, fp, fn = calcaulateConfusionMatrix(yTrue, yPredicted)\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    return (2 * precision * recall) / (precision + recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A classifier which always output the level of majority class as predicted label \n",
    "def majorityClassBaselineClassifier(xTrain, yTrain, xTest):\n",
    "    if np.count_nonzero(yTrain) >= (len(yTrain) - np.count_nonzero(yTrain)):\n",
    "        return np.ones(len(xTest))\n",
    "    return np.zeros(len(xTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P, R, F for trainData::: Precision: 0.500000, Recall: 1.000000, and F-score: 0.666667 \n",
      "\n",
      "P, R, F for testData::: Precision: 0.500000, Recall: 1.000000, and F-score: 0.666667 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#predict the label using Training data as Test data\n",
    "prediction = majorityClassBaselineClassifier(xTrain, yTrain, xTrain)\n",
    "print(\"P, R, F for trainData::: Precision: %f, Recall: %f, and F-score: %f \\n\"%(calculatePrecision(yTrain, prediction)\n",
    "                                                                          , calculateRecall(yTrain, prediction)\n",
    "                                                                        , calculateFscore(yTrain, prediction)))\n",
    "#predict the label using Training data as Test data  \n",
    "prediction = majorityClassBaselineClassifier(xTrain, yTrain, xTest)\n",
    "print(\"P, R, F for testData::: Precision: %f, Recall: %f, and F-score: %f \\n\"%(calculatePrecision(yTest, prediction)\n",
    "                                                                          , calculateRecall(yTest, prediction)\n",
    "                                                                          , calculateFscore(yTest, prediction)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A classifier which predicts the label based on the review length\n",
    "def reviewLengthBaselineClassifier(x_train, x_test, threshold):\n",
    "    prediction = np.array([], dtype='object')\n",
    "    for x in x_test:\n",
    "        if len(x) > threshold:\n",
    "            prediction = np.append(prediction, 1)\n",
    "        else:\n",
    "            prediction = np.append(prediction, 0)       \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P, R, F for trainData with threshold 60::: Precision: 0.500040, Recall: 1.000000, and F-score: 0.666702 \n",
      "\n",
      "P, R, F for testData with threshold 60::: Precision: 0.500080, Recall: 1.000000, and F-score: 0.666738 \n",
      "\n",
      "P, R, F for trainData with threshold 80::: Precision: 0.500080, Recall: 0.999920, and F-score: 0.666720 \n",
      "\n",
      "P, R, F for testData with threshold 80::: Precision: 0.500060, Recall: 0.999840, and F-score: 0.666684 \n",
      "\n",
      "P, R, F for trainData with threshold 400::: Precision: 0.494995, Recall: 0.925760, and F-score: 0.645075 \n",
      "\n",
      "P, R, F for testData with threshold 400::: Precision: 0.496517, Recall: 0.923760, and F-score: 0.645878 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction = reviewLengthBaselineClassifier(xTrain, xTrain, 60)\n",
    "print(\"P, R, F for trainData with threshold 60::: Precision: %f, Recall: %f, and F-score: %f \\n\"%(calculatePrecision(yTrain, prediction)\n",
    "                                                                          , calculateRecall(yTrain, prediction)\n",
    "                                                                          , calculateFscore(yTrain, prediction)))\n",
    "prediction = reviewLengthBaselineClassifier(xTrain, xTest, 60)\n",
    "print(\"P, R, F for testData with threshold 60::: Precision: %f, Recall: %f, and F-score: %f \\n\"%(calculatePrecision(yTest, prediction)\n",
    "                                                                          , calculateRecall(yTest, prediction)\n",
    "                                                                          , calculateFscore(yTest, prediction)))\n",
    "prediction = reviewLengthBaselineClassifier(xTrain, xTrain, 80)\n",
    "print(\"P, R, F for trainData with threshold 80::: Precision: %f, Recall: %f, and F-score: %f \\n\"%(calculatePrecision(yTrain, prediction)\n",
    "                                                                          , calculateRecall(yTrain, prediction)\n",
    "                                                                          , calculateFscore(yTrain, prediction)))\n",
    "prediction = reviewLengthBaselineClassifier(xTrain, xTest, 80)\n",
    "print(\"P, R, F for testData with threshold 80::: Precision: %f, Recall: %f, and F-score: %f \\n\"%(calculatePrecision(yTest, prediction)\n",
    "                                                                          , calculateRecall(yTest, prediction)\n",
    "                                                                          , calculateFscore(yTest, prediction)))\n",
    "prediction = reviewLengthBaselineClassifier(xTrain, xTrain, 400)\n",
    "print(\"P, R, F for trainData with threshold 400::: Precision: %f, Recall: %f, and F-score: %f \\n\"%(calculatePrecision(yTrain, prediction)\n",
    "                                                                          , calculateRecall(yTrain, prediction)\n",
    "                                                                          , calculateFscore(yTrain, prediction)))\n",
    "prediction = reviewLengthBaselineClassifier(xTrain, xTest, 400)\n",
    "print(\"P, R, F for testData with threshold 400::: Precision: %f, Recall: %f, and F-score: %f \\n\"%(calculatePrecision(yTest, prediction)\n",
    "                                                                          , calculateRecall(yTest, prediction)\n",
    "                                                                          , calculateFscore(yTest, prediction)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label will be generated based on the rating present in labeledBow.feat file\n",
    "def generateLabel(data):\n",
    "    label = []\n",
    "    for d in data:\n",
    "        if d > 5:\n",
    "            label.append(1)\n",
    "        else:\n",
    "            label.append(0)\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P, R, F for training Data::: Precision: 0.992216, Recall: 0.846400, and F-score: 0.913526 \n",
      "\n",
      "P, R, F for testing Data::: Precision: 0.624640, Recall: 0.433200, and F-score: 0.511597 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### Naive bayes classifier-GaussianNB with features from labeledBow.feat file\n",
    "\n",
    "bowFilePath = [\"aclImdb/train/labeledBow.feat\",\"aclImdb/test/labeledBow.feat\"]\n",
    "xTraining, yTraining, xTesting, yTesting = load_svmlight_files(bowFilePath, n_features=None, dtype=None)\n",
    "xTraining = np.int16(xTraining.todense())\n",
    "xTesting = np.int16(xTesting.todense())\n",
    "labelTraing = generateLabel(yTraining)\n",
    "labelTesting = generateLabel(yTesting)\n",
    "naiveBayesModel = GaussianNB().fit(xTraining, labelTraing)\n",
    "\n",
    "predicted= naiveBayesModel.predict(xTraining)\n",
    "print(\"P, R, F for training Data::: Precision: %f, Recall: %f, and F-score: %f \\n\"\n",
    "      %(calculatePrecision(labelTraing, predicted), calculateRecall(labelTraing, predicted)\n",
    "        , calculateFscore(labelTraing, predicted)))\n",
    "\n",
    "predicted= naiveBayesModel.predict(xTesting)\n",
    "print(\"P, R, F for testing Data::: Precision: %f, Recall: %f, and F-score: %f \\n\"\n",
    "      %(calculatePrecision(labelTesting, predicted), calculateRecall(labelTesting, predicted)\n",
    "        , calculateFscore(labelTesting, predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Naive bayes classifier-GaussianNB with features from labeledBow.feat file \n",
    "### and at the end transforming it with TfidfTransformer\n",
    "\n",
    "tf_transformer = TfidfTransformer()\n",
    "xTraining1 = tf_transformer.fit_transform(xTraining)\n",
    "xTesting1 = tf_transformer.fit_transform(xTesting)\n",
    "\n",
    "naiveBayesModel1 = GaussianNB().fit(xTraining1.toarray(), labelTraing)\n",
    "\n",
    "predicted= naiveBayesModel1.predict(xTraining1.toarray())\n",
    "print(\"P, R, F for training Data::: Precision: %f, Recall: %f, and F-score: %f \\n\"\n",
    "      %(calculatePrecision(labelTraing, predicted), calculateRecall(labelTraing, predicted)\n",
    "        , calculateFscore(labelTraing, predicted)))\n",
    "\n",
    "predicted= naiveBayesModel1.predict(xTesting1.toarray())\n",
    "print(\"P, R, F for testing Data::: Precision: %f, Recall: %f, and F-score: %f \\n\"\n",
    "      %(calculatePrecision(labelTesting, predicted), calculateRecall(labelTesting, predicted)\n",
    "        , calculateFscore(labelTesting, predicted)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P, R, F for training Data::: Precision: 0.665659, Recall: 0.704960, and F-score: 0.684746 \n",
      "\n",
      "P, R, F for testing Data::: Precision: 0.664780, Recall: 0.704560, and F-score: 0.684092 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "##### naive bayes classifier-GaussianNB with features calculation using TfidfVectorizer\n",
    "\n",
    "trainingData = np.concatenate((positiveTrainReviwList, negativeTrainReviwList), axis=None)\n",
    "testingData = np.concatenate((positiveTrainReviwList, negativeTestReviwList), axis=None)\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=2, tokenizer=nltk.word_tokenize, max_features=100)\n",
    "\n",
    "trainingData = vectorizer.fit_transform(trainingData)\n",
    "testingData = vectorizer.fit_transform(testingData)\n",
    "\n",
    "naiveBayesModel2 = GaussianNB().fit(trainingData.toarray(), yTrain)\n",
    "\n",
    "predicted= naiveBayesModel2.predict(trainingData.toarray())\n",
    "print(\"P, R, F for training Data::: Precision: %f, Recall: %f, and F-score: %f \\n\"\n",
    "      %(calculatePrecision(yTrain, predicted), calculateRecall(yTrain, predicted)\n",
    "        , calculateFscore(yTrain, predicted)))\n",
    "\n",
    "predicted= naiveBayesModel2.predict(testingData.toarray())\n",
    "print(\"P, R, F for testing Data::: Precision: %f, Recall: %f, and F-score: %f \\n\"\n",
    "      %(calculatePrecision(yTest, predicted), calculateRecall(yTest, predicted)\n",
    "        , calculateFscore(yTest, predicted)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P, R, F for training Data::: Precision: 0.926899, Recall: 0.937280, and F-score: 0.932060 \n",
      "\n",
      "P, R, F for testing Data::: Precision: 0.881565, Recall: 0.884880, and F-score: 0.883220 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### LogisticRegression with features from labeledBow.feat file and at the end transforming it with TfidfTransformer\n",
    "## xTraining and xTesting are calculated by extrating features from from labeledBow.feat file \n",
    "## and  transforming it with TfidfTransformer\n",
    "\n",
    "lRModel = LogisticRegression().fit(xTraining.toarray(), labelTraing)\n",
    "\n",
    "predicted= lRModel.predict(xTraining.toarray())\n",
    "print(\"P, R, F for training Data::: Precision: %f, Recall: %f, and F-score: %f \\n\"\n",
    "      %(calculatePrecision(labelTraing, predicted), calculateRecall(labelTraing, predicted)\n",
    "        , calculateFscore(labelTraing, predicted)))\n",
    "\n",
    "predicted= lRModel.predict(xTesting.toarray())\n",
    "print(\"P, R, F for testing Data::: Precision: %f, Recall: %f, and F-score: %f \\n\"\n",
    "      %(calculatePrecision(labelTesting, predicted), calculateRecall(labelTesting, predicted)\n",
    "        , calculateFscore(labelTesting, predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P, R, F for training Data::: Precision: 0.708044, Recall: 0.726000, and F-score: 0.716910 \n",
      "\n",
      "P, R, F for testing Data::: Precision: 0.701471, Recall: 0.725040, and F-score: 0.713061 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### LogisticRegression with features calculation using TfidfVectorizer\n",
    "## trainingData and testingData are calculated by extrating features using TfidfVectorizer\n",
    "\n",
    "lRModel1 = LogisticRegression().fit(trainingData.toarray(), labelTraing)\n",
    "\n",
    "predicted= lRModel1.predict(trainingData.toarray())\n",
    "print(\"P, R, F for training Data::: Precision: %f, Recall: %f, and F-score: %f \\n\"\n",
    "      %(calculatePrecision(labelTraing, predicted), calculateRecall(labelTraing, predicted)\n",
    "        , calculateFscore(labelTraing, predicted)))\n",
    "\n",
    "predicted= lRModel1.predict(testingData.toarray())\n",
    "print(\"P, R, F for testing Data::: Precision: %f, Recall: %f, and F-score: %f \\n\"\n",
    "      %(calculatePrecision(labelTesting, predicted), calculateRecall(labelTesting, predicted)\n",
    "        , calculateFscore(labelTesting, predicted)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    ":::trainData::: Precision: 0.500000, Recall: 1.000000, and F-score: 0.666667 <br>\n",
    ":::testData::: Precision: 0.500000, Recall: 1.000000, and F-score: 0.666667 \n",
    "since number of positive and negative reviews are equal for both test data and train data, the precision is reasonable.\n",
    "\n",
    "## Problem 3\n",
    ":::trainData with threshold 60::: Precision: 0.500040, Recall: 1.000000, and F-score: 0.666702 <br>\n",
    ":::testData with threshold 60::: Precision: 0.500080, Recall: 1.000000, and F-score: 0.666738 <br>\n",
    "\n",
    ":::trainData with threshold 80::: Precision: 0.500080, Recall: 0.999920, and F-score: 0.666720 <br>\n",
    ":::testData with threshold 80::: Precision: 0.500060, Recall: 0.999840, and F-score: 0.666684 <br>\n",
    "\n",
    ":::trainData with threshold 400::: Precision: 0.494995, Recall: 0.925760, and F-score: 0.645075 <br>\n",
    ":::testData with threshold 400::: Precision: 0.496517, Recall: 0.923760, and F-score: 0.645878 \n",
    "\n",
    "<br>From the result, it seems that all of the positive review length are less than 400. and negative review length are greater than or equal 400.\n",
    "\n",
    "## Problem 4\n",
    "#### Naive bayes classifier-GaussianNB with features from labeledBow.feat file\n",
    ":::training Data::: Precision: 0.992216, Recall: 0.846400, and F-score: 0.913526  \n",
    ":::testing Data::: Precision: 0.624640, Recall: 0.433200, and F-score: 0.511597 \n",
    "\n",
    "#### Naive bayes classifier-GaussianNB with features from labeledBow.feat file and at the end transforming it with TfidfTransformer\n",
    ":::training Data::: Precision: 0.993969, Recall: 0.949360, and F-score: 0.971153 <br> \n",
    ":::testing Data::: Precision: 0.608667, Recall: 0.521360, and F-score: 0.561641\n",
    "\n",
    "#### Naive bayes classifier-GaussianNB with features calculation using TfidfVectorizer\n",
    ":::training Data::: Precision: 0.665659, Recall: 0.704960, and F-score: 0.684746 <br> \n",
    ":::testing Data::: Precision: 0.664780, Recall: 0.704560, and F-score: 0.684092 \n",
    "\n",
    "<br>From the analysis of above presented results of 3 cases for GaussianNB classifier, it is clear that the classifier gives better results when features are calculated using already calculated BOW feature file provided with dataset- labeledBow.feat and transforming it to TFIDF features.\n",
    "<br>From the reported result, GaussianNB classifier with TfidfVectorizer gives worst result if stop words are removed, strips_accent ='unicode' and  ngram range is (1,3), and gives best result when the features are used from labeledBow.feat file.\n",
    "Though using TfidfVectorizer gives best result for test data, the precision of traning data drops from 99% to 69%. One reason for this can be the removal of stop word and accent may remove some of the impotant freatures of reviews. At th end i removed the stop word removing, accent removing and set to use unigram model for TfidfVectorizer. In this case, the precision improves a little, however still not much to consider.\n",
    "\n",
    "## Problem 5\n",
    "#### LogisticRegression with features from labeledBow.feat file and at the end transforming it with TfidfTransformer\n",
    ":::training Data::: Precision: 0.926899, Recall: 0.937280, and F-score: 0.932060 <br> \n",
    ":::testing Data::: Precision: 0.881565, Recall: 0.884880, and F-score: 0.883220 \n",
    "\n",
    "#### LogisticRegression with features calculation using TfidfVectorizer\n",
    ":::training Data::: Precision: 0.708044, Recall: 0.726000, and F-score: 0.716910<br> \n",
    ":::testing Data::: Precision: 0.701471, Recall: 0.725040, and F-score: 0.713061 \n",
    "\n",
    "<br> Like the problem 4 the logistic regression gives best precision if the features are used from labeledBow.feat file and transformed it TFIDF features.\n",
    " \n",
    "<br> For the 2 cases of Logistic Regression, it also gives better results when features are calculated using already calculated BOW feature file provided with dataset- labeledBow.feat and transforming it to TFIDF features.<br> \n",
    " Among the 2 classifier logistic regression classifier gives much better results with accuracy of around 88% and recall of 88.5%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the word2vec pretrained model from google news dataset\n",
    "filename = 'GoogleNews-vectors-negative300.bin'\n",
    "googleModel = KeyedVectors.load_word2vec_format(filename, binary=True, limit=500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the globe word2vec pretrained model\n",
    "glove_input_file = 'glove.6B.100d.txt'\n",
    "word2vec_output_file = 'glove.6B.100d.txt.word2vec'\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)\n",
    "filename = 'glove.6B.100d.txt.word2vec'\n",
    "gloveModel = KeyedVectors.load_word2vec_format(filename, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find out top word for a analogy test for all given category\n",
    "def testAanalogy(modelName, model, analogySet):\n",
    "    groupAccuracy = {}\n",
    "    highestSimilarWord = {}\n",
    "    for key in analogySet:\n",
    "        TP = 0\n",
    "        failed = 0\n",
    "        for analogy in analogySet[key]:\n",
    "            if all(word in model for word in analogy):\n",
    "                highestSimilarWord = model.most_similar(positive = [analogy[1], analogy[2]]\n",
    "                                                        , negative = [analogy[0]], topn=1)\n",
    "                print(highestSimilarWord[0][0])\n",
    "                if highestSimilarWord[0][0] == analogy[3]:\n",
    "                    TP += 1\n",
    "            else:\n",
    "                # Failed prediction because of vocabulary absence\n",
    "                if failed == (len(analogySet[key]) - 1):\n",
    "                    print(\"One/more Vocabulary missing for this analogy: \", analogy)\n",
    "                failed += 1\n",
    "\n",
    "        \n",
    "        #print(\"Category:\", key, \", Failed:\", failed, \", Total:\", len(analogySet[key]), (len(analogySet[key]) - failed), \", TP:\", TP)              \n",
    "        if len(analogySet[key]) == failed:\n",
    "            groupAccuracy[key] = 0\n",
    "        else:\n",
    "            groupAccuracy[key] = TP / (len(analogySet[key]) - failed)\n",
    "    print(modelName, \":\\n\", groupAccuracy, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 7 : Reporting the Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glove pretrained model on Wikipedia :\n",
      " {': capital-world': 0.8894783377541998, ': currency': 0.1420323325635104, ': city-in-state': 0.3080664775030401, ': family': 0.8162055335968379, ': gram1-adjective-to-adverb': 0.2439516129032258, ': gram2-opposite': 0.20073891625615764, ': gram3-comparative': 0.7912912912912913, ': gram6-nationality-adjective': 0.8786741713570981} \n",
      "\n",
      "Google Word2Vec 300d news data set :\n",
      " {': capital-world': 0.2, ': currency': 0.17222222222222222, ': city-in-state': 0.16326530612244897, ': family': 0.8517786561264822, ': gram1-adjective-to-adverb': 0.2923387096774194, ': gram2-opposite': 0.42980295566502463, ': gram3-comparative': 0.9114114114114115, ': gram6-nationality-adjective': 0.5775862068965517} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "##Read Mikelov data set and preprocess it\n",
    "analogyDataSet = requests.get(\"http://www.fit.vutbr.cz/~imikolov/rnnlm/word-test.v1.txt\")\n",
    "analogyDataSet = analogyDataSet.text.split(\"\\n\")\n",
    "\n",
    "categoryDict = {}\n",
    "for item in analogyDataSet[1:]:\n",
    "    if \":\" in item:\n",
    "        cKey = item.strip().lower()\n",
    "        categoryDict[cKey] = []\n",
    "    else:\n",
    "        categoryDict[cKey].append(item.strip().lower().split())   \n",
    "        \n",
    "requiredGroupList = [': capital-world', ': currency', ': city-in-state', ': family',\n",
    "': gram1-adjective-to-adverb', ': gram2-opposite', ': gram3-comparative', ': gram6-nationality-adjective']\n",
    "requiredGroupAnalogy = {}\n",
    "for rGroup in requiredGroupList:\n",
    "    requiredGroupAnalogy[rGroup] = categoryDict[rGroup]\n",
    "\n",
    "# Report the accuracy of the mentioned analogy \n",
    "testAanalogy(\"Glove pretrained model on Wikipedia\", gloveModel, requiredGroupAnalogy)\n",
    "testAanalogy(\"Google Word2Vec 300d news data set\", googleModel, requiredGroupAnalogy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, I used word2Vec pretrained model provied by google (googel news 300d)[2] and Glove pretrained model with 6B token 100d dataset[1]. For the required. to Run the program, one have to put the  'GoogleNews-vectors-negative300.bin' and 'glove.6B.100d.txt' pretrianed model in the same golder where this ipython notebook resides.\n",
    "\n",
    "The accurracy for each group is reported in the above execution. Analysis the analogy accuracy reported by both glove and word2Vec, the glove word embedding out performs world2vec in the analogy prediction task. Moreover, word2vec model pretrained model built on google news data does not contain all of the vocabulary exist int he mikhelov analogy set. One reason for missing vocabulary is that i load only 500k frequently used word vector for google news word embeding pretrained model. For the accuracy calculation, if a analogy prediction task failed for missing vocabulary, i did not consider this anlogy entry in the total prediction count.\n",
    "\n",
    "For analogy category {capital-world, city-in-state, gram6-nationality-adjective} glove has high accuracy whereas, for\n",
    "analogy category {currency, family, gram1-adjective-to-adverb, gram2-opposite, gram3-comparative} word2Vec embedding has high accuracy. The analysis reveals that the word2Vec outperforms glove in most of the mikelov analogy test(except one- gram6-nationality-adjective) cases if the vocabulary is present although i used only the 500k frequently used vector for word2Vec.\n",
    "\n",
    "1. Glove pretrained model, 6B 100d, https://nlp.stanford.edu/projects/glove/\n",
    "2. Google news word2vec model, https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('increased', 0.8988391160964966), ('increases', 0.8812485933303833), ('decrease', 0.8812355995178223), ('increasing', 0.8467099070549011), ('reduce', 0.8099488019943237), ('rise', 0.7936252951622009), ('reduced', 0.7800052165985107), ('growth', 0.7789974212646484), ('reduction', 0.7766253352165222), ('boost', 0.7759016752243042)] \n",
      "\n",
      "[('hostility', 0.7843601703643799), ('bigotry', 0.7682587504386902), ('intolerance', 0.7667320966720581), ('animosity', 0.756497859954834), ('resentment', 0.7466510534286499), ('enmity', 0.7424829006195068), ('bitterness', 0.7228323817253113), ('prejudice', 0.7177624106407166), ('anger', 0.7149417400360107), ('fanaticism', 0.7136397957801819)] \n",
      "\n",
      "[('younger', 0.828304648399353), ('age', 0.7673466205596924), ('newer', 0.688201904296875), ('ages', 0.6771453022956848), ('those', 0.6731763482093811), ('siblings', 0.670750081539154), ('parents', 0.6635492444038391), ('children', 0.6634655594825745), ('ones', 0.6595342755317688), ('young', 0.6554997563362122)] \n",
      "\n",
      "[('decrease', 0.8370318412780762), ('increases', 0.7709376811981201), ('increased', 0.7578041553497314), ('reduction', 0.6908220648765564), ('increasing', 0.6871615648269653), ('decreases', 0.6816173791885376), ('rise', 0.6352647542953491), ('decreasing', 0.6218624114990234), ('decline', 0.6128641366958618), ('uptick', 0.5923734903335571)] \n",
      "\n",
      "[('hostility', 0.6782528162002563), ('bigotry', 0.6700395345687866), ('animosity', 0.6674726605415344), ('resentment', 0.6527854800224304), ('enmity', 0.6499381065368652), ('animus', 0.6490738391876221), ('racial_hatred', 0.6462260484695435), ('loathing', 0.6460458040237427), ('detestation', 0.6383641958236694), ('bitterness', 0.6343967318534851)] \n",
      "\n",
      "[('younger', 0.8020765781402588), ('Older', 0.6751844882965088), ('Younger', 0.6121368408203125), ('newer', 0.5612284541130066), ('age', 0.5566693544387817), ('aged', 0.5119142532348633), ('aging', 0.5045050978660583), ('ages', 0.48286187648773193), ('young', 0.474964439868927), ('chubbier', 0.4667407274246216)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find out top 10 similar word of 'increase', 'hatred' and 'older'\n",
    "print(gloveModel.most_similar(\"increase\", topn=10), \"\\n\")\n",
    "print(gloveModel.most_similar(\"hatred\", topn=10), \"\\n\")\n",
    "print(gloveModel.most_similar(\"older\", topn=10), \"\\n\")\n",
    "print(googleModel.most_similar(\"increase\", topn=10), \"\\n\")\n",
    "print(googleModel.most_similar(\"hatred\", topn=10), \"\\n\")\n",
    "print(googleModel.most_similar(\"older\", topn=10), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above executed program, the top 10 similar words of \"increase\" and \"older\" contains their antonym. However, the top 10 similar words of  \"hatred\" does not contain any anotonyms. One reason for a antonym's similarty score to be higher is that both the word and it's antonym can be used in same context in a sentence. For example, \"older boy\" or \"younger boy\", in this two example both older and younger are associated with boy as an adjective. In another word, the use of paradigmetic relation in word2vec makes it to have high similarity score of an antonym of a word becasue both of them can be used in the same context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "players\n",
      "work\n",
      "and\n",
      "Glove pretrained model on Wikipedia :\n",
      " {'performar_action': 0.0} \n",
      "\n",
      "blue\n",
      "darryl\n",
      "red\n",
      "Glove pretrained model on Wikipedia :\n",
      " {'object_color': 0.0} \n",
      "\n",
      "factory\n",
      "wig\n",
      "vehicle\n",
      "Glove pretrained model on Wikipedia :\n",
      " {'object_organ': 0.0} \n",
      "\n",
      "players\n",
      "artists\n",
      "Google Word2Vec 300d news data set :\n",
      " {'performar_action': 0.0} \n",
      "\n",
      "yellow\n",
      "strawberries\n",
      "red\n",
      "Google Word2Vec 300d news data set :\n",
      " {'object_color': 0.0} \n",
      "\n",
      "plants\n",
      "tresses\n",
      "crucial\n",
      "Google Word2Vec 300d news data set :\n",
      " {'object_organ': 0.0} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Designing three new analogy and predict it using Glove and Word2vec word embedding\n",
    "analogy1_performer_action = {}\n",
    "analogy2_object_color = {}\n",
    "analogy3_object_organ =  {}\n",
    "analogy1_performer_action[\"performar_action\"] = [[\"teacher\", \"teach\", \"player\", \"play\"],\n",
    "                                [\"footballer\", \"play\", \"artist\", \"paint\"],\n",
    "                                [\"helpe\", \"help\", \"singer\", \"sing\"]]\n",
    "\n",
    "analogy2_object_color[\"object_color\"] = [['tomato', 'red', 'cabbage', 'green'],\n",
    "                            [\"orange\", \"yellow\", \"strawberry\", \"red\"],\n",
    "                            [\"lemon\", \"yellow\", \"lime\", \"green\"]]\n",
    "analogy3_object_organ[\"object_organ\"] = [['human', 'hand', 'plant', 'leaf'],\n",
    "                            [\"human\", \"hair\", \"computer\", \"display\"],\n",
    "                            [\"keyboard\", \"key\", \"car\", \"tire\"]]\n",
    "\n",
    "testAanalogy(\"Glove pretrained model on Wikipedia\", gloveModel, analogy1_performer_action)\n",
    "testAanalogy(\"Glove pretrained model on Wikipedia\", gloveModel, analogy2_object_color)\n",
    "testAanalogy(\"Glove pretrained model on Wikipedia\", gloveModel, analogy3_object_organ)\n",
    "\n",
    "testAanalogy(\"Google Word2Vec 300d news data set\", googleModel, analogy1_performer_action)\n",
    "testAanalogy(\"Google Word2Vec 300d news data set\", googleModel, analogy2_object_color)\n",
    "testAanalogy(\"Google Word2Vec 300d news data set\", googleModel, analogy3_object_organ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above execution of three analogy categories, both the glove and word2Vec provide zero accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirement to Run the File\n",
    "The aclIMDB dataset, glove.6B.100d.txt and GoogleNews-vectors-negative300.bin needs to be in the same folder as of the jupyter notebook file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
