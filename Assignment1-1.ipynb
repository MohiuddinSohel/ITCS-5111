{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import udhr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#English data set\n",
    "english = udhr.raw('English-Latin1')\n",
    "english_train, english_dev, english_test = english[0:1000], english[1000:1100], udhr.words('English-Latin1')[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#French data set\n",
    "french = udhr.raw('French_Francais-Latin1')\n",
    "french_train, french_dev, french_test = french[0:1000], french[1000:1100], udhr.words('French_Francais-Latin1')[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Italian data set\n",
    "italian = udhr.raw('Italian_Italiano-Latin1')\n",
    "italian_train, italian_dev, italian_test = italian[0:1000], italian[1000:1100], udhr.words('Italian_Italiano-Latin1')[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spanish data set\n",
    "spanish = udhr.raw('Spanish_Espanol-Latin1')\n",
    "spanish_train, spanish_dev, spanish_test = spanish[0:1000], spanish[1000:1100], udhr.words('Spanish_Espanol-Latin1')[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Convert the raw text into nGram by using characterList\n",
    "def buildCharacterNGram(rawText, nGramN):\n",
    "    wordList = nltk.word_tokenize(rawText.lower())\n",
    "    nGramList = []\n",
    "    for word in wordList:\n",
    "        nGrams = nltk.ngrams(word, nGramN)#, pad_left='True', pad_right='True'\n",
    "        for nGram in nGrams:\n",
    "            nGramList.append(nGram)\n",
    "    return nGramList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate frequancy distribution any n-gram\n",
    "def calculateFrequencyDistributionOfEachNgram(nGramList, nGramN):\n",
    "    if nGramN == 1 :\n",
    "        frequencyDistribution = nltk.FreqDist(nGramList)\n",
    "    else:\n",
    "        if nGramN == 2 :\n",
    "            frequencyDistribution = nltk.ConditionalFreqDist((((t0), t1) for t0, t1 in nGramList))\n",
    "        elif nGramN == 3 :\n",
    "            frequencyDistribution = nltk.ConditionalFreqDist((((t0, t1), t2) for t0, t1, t2 in nGramList))\n",
    "        else:\n",
    "            print(\"The nGram parameter is wrong. Unigram, bigram and trigram are supported now\")\n",
    "    return frequencyDistribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate unigam probability distribution\n",
    "def calculateUnigramProbabilityDistribution(rawText, nGramN):\n",
    "    \n",
    "    nGramList = buildCharacterNGram(rawText, nGramN)\n",
    "    frequencyDistribution = calculateFrequencyDistributionOfEachNgram(nGramList, nGramN)\n",
    "    unigramFList = {}\n",
    "    probabilityDistribution = {}\n",
    "    totalChar = 0\n",
    "    for key, value in frequencyDistribution.items():\n",
    "        totalChar += frequencyDistribution[key]\n",
    "        unigramFList[key[0]] = value\n",
    "        \n",
    "    for key, value in frequencyDistribution.items():\n",
    "        probabilityDistribution[key[0]] = frequencyDistribution[key] / totalChar\n",
    "    #print(probabilityDistribution)\n",
    "    return (unigramFList, totalChar, probabilityDistribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate biigam probability distribution\n",
    "def calculateBigramProbabilityDistribution(rawText, nGramN, unigramFrequencyDistribution):\n",
    "\n",
    "    nGramList = buildCharacterNGram(rawText, nGramN)\n",
    "    bigramFrequencyDistribution = calculateFrequencyDistributionOfEachNgram(nGramList, nGramN)\n",
    "    \n",
    "    \n",
    "    bigramFList = {}\n",
    "    for key, value in bigramFrequencyDistribution.items():\n",
    "        for k,v in value.items():\n",
    "            bigramFList[key, k] = v;\n",
    "\n",
    "    probabilityDistribution = {}\n",
    "    for key, value in bigramFrequencyDistribution.items():\n",
    "        for k,v in value.items():\n",
    "            probabilityDistribution[key, k] = bigramFrequencyDistribution[key][k] / unigramFrequencyDistribution[k]\n",
    "            \n",
    "    #print(probabilityDistribution)\n",
    "    return (bigramFList, probabilityDistribution)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate trigam probability distribution\n",
    "def calculateTrigramProbabilityDistribution(rawText, nGramN, \n",
    "                                            bigramFrequencyDistribution, unigramFrequencyDistribution):\n",
    "    \n",
    "    nGramList = buildCharacterNGram(rawText, nGramN)\n",
    "    trigramFrequencyDistribution = calculateFrequencyDistributionOfEachNgram(nGramList, nGramN)\n",
    "    \n",
    "    probabilityDistribution = {}\n",
    "    for key,  valueList   in trigramFrequencyDistribution.items():\n",
    "        for k, values in valueList.items():\n",
    "            probabilityDistribution[key[0],key[1], k] = values / bigramFrequencyDistribution[key[0],key[1]]\n",
    "    #print(probabilityDistribution)\n",
    "    return probabilityDistribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the probability of a word using trained language mode and Back off smoothing\n",
    "def calculateProbabilityOfTesData(word, nGramN, trainedModel, lang):\n",
    "    predictedProbability = 1\n",
    "    if nGramN == 1:\n",
    "        for character in word:\n",
    "            if character in trainedModel[lang, \"unigram\", \"probability\"]:\n",
    "                predictedProbability *= trainedModel[lang, \"unigram\", \"probability\"][character]\n",
    "            else:\n",
    "                predictedProbability *= (1 / trainedModel[lang, \"unigram\", \"total\"])\n",
    "                #print(\"Character misssing: \" , character)\n",
    "    elif nGramN == 2:\n",
    "        nGramlist = buildCharacterNGram(word, nGramN)\n",
    "        for bigram in nGramlist:\n",
    "            if bigram in trainedModel[lang, \"bigram\", \"probability\"]:\n",
    "                predictedProbability *= trainedModel[lang, \"bigram\", \"probability\"][bigram]\n",
    "            else:\n",
    "                if bigram[0] in trainedModel[lang, \"unigram\", \"probability\"]:\n",
    "                    predictedProbability *= trainedModel[lang, \"unigram\", \"probability\"][bigram[0]]\n",
    "                else:\n",
    "                    predictedProbability *= (1 / trainedModel[lang, \"unigram\", \"total\"])\n",
    "                #print(\"bigram Missing : \", bigram[0], bigram[1])\n",
    "\n",
    "    elif nGramN == 3:\n",
    "        nGramlist = buildCharacterNGram(word, nGramN)\n",
    "        for trigram in nGramlist:\n",
    "            if trigram in trainedModel[lang, \"trigram\", \"probability\"]:\n",
    "                predictedProbability *= trainedModel[lang, \"trigram\", \"probability\"][trigram]\n",
    "            else:\n",
    "                if(trigram[0], trigram[1]) in trainedModel[lang, \"bigram\", \"probability\"]:\n",
    "                    predictedProbability *= trainedModel[lang, \"bigram\", \"probability\"][(trigram[0], trigram[1])]\n",
    "                    #print(\"missing: last-2\")\n",
    "                else:\n",
    "                    if trigram[0] in trainedModel[lang, \"unigram\", \"probability\"]:\n",
    "                        predictedProbability *= trainedModel[lang, \"unigram\", \"probability\"][trigram[0]]\n",
    "                        #print(\"missing: last-1\")\n",
    "                    else: \n",
    "                        predictedProbability *= (1 / trainedModel[lang, \"unigram\", \"total\"])\n",
    "                        #print(\"missing: last\")\n",
    "\n",
    "    else:\n",
    "        print(\"The nGram parameter is wrong. Unigram, bigram and Trigram are supported now\")\n",
    "    return predictedProbability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train and generate the language model for all 4 languages\n",
    "def generateAndStoreTrainedLanguageModel():\n",
    "    trainedModel = dict()\n",
    "    frqUnigramE, total, probUnigramE = calculateUnigramProbabilityDistribution(english_train, 1)\n",
    "    trainedModel[\"E\", \"unigram\", \"frequency\"] = frqUnigramE\n",
    "    trainedModel[\"E\", \"unigram\", \"total\"] = total\n",
    "    trainedModel[\"E\", \"unigram\", \"probability\"] = probUnigramE\n",
    "    frqBigramE, probBigramE = calculateBigramProbabilityDistribution(english_train, 2, frqUnigramE)\n",
    "    trainedModel[\"E\", \"bigram\", \"frequency\"] = frqBigramE\n",
    "    trainedModel[\"E\", \"bigram\", \"probability\"] = probBigramE\n",
    "    probTrigramE = calculateTrigramProbabilityDistribution(english_train, 3, frqBigramE, frqUnigramE)\n",
    "    trainedModel[\"E\", \"trigram\", \"probability\"] = probTrigramE\n",
    "    \n",
    "    frqUnigramF, total, probUnigramF = calculateUnigramProbabilityDistribution(french_train, 1)\n",
    "    trainedModel[\"F\", \"unigram\", \"frequency\"] = frqUnigramF\n",
    "    trainedModel[\"F\", \"unigram\", \"total\"] = total\n",
    "    trainedModel[\"F\", \"unigram\", \"probability\"] = probUnigramF\n",
    "    frqBigramF, probBigramF = calculateBigramProbabilityDistribution(french_train, 2, frqUnigramF)\n",
    "    trainedModel[\"F\", \"bigram\", \"frequency\"] = frqBigramF\n",
    "    trainedModel[\"F\", \"bigram\", \"probability\"] = probBigramF\n",
    "    probTrigramF = calculateTrigramProbabilityDistribution(french_train, 3, frqBigramF, frqUnigramF)\n",
    "    trainedModel[\"F\", \"trigram\", \"probability\"] = probTrigramF\n",
    "\n",
    "    frqUnigramI, total, probUnigramI = calculateUnigramProbabilityDistribution(italian_train, 1)\n",
    "    trainedModel[\"I\", \"unigram\", \"frequency\"] = frqUnigramI\n",
    "    trainedModel[\"I\", \"unigram\", \"total\"] = total\n",
    "    trainedModel[\"I\", \"unigram\", \"probability\"] = probUnigramI\n",
    "    frqBigramI, probBigramI = calculateBigramProbabilityDistribution(italian_train, 2, frqUnigramI)\n",
    "    trainedModel[\"I\", \"bigram\", \"frequency\"] = frqBigramI\n",
    "    trainedModel[\"I\", \"bigram\", \"probability\"] = probBigramI\n",
    "    probTrigramI = calculateTrigramProbabilityDistribution(italian_train, 3, frqBigramI, frqUnigramI)\n",
    "    trainedModel[\"I\", \"trigram\", \"probability\"] = probTrigramI\n",
    "    \n",
    "    frqUnigramS, total, probUnigramS = calculateUnigramProbabilityDistribution(spanish_train, 1)\n",
    "    trainedModel[\"S\", \"unigram\", \"frequency\"] = frqUnigramS\n",
    "    trainedModel[\"S\", \"unigram\", \"total\"] = total\n",
    "    trainedModel[\"S\", \"unigram\", \"probability\"] = probUnigramS\n",
    "    frqBigramS, probBigramS = calculateBigramProbabilityDistribution(spanish_train, 2, frqUnigramS)\n",
    "    trainedModel[\"S\", \"bigram\", \"frequency\"] = frqBigramS\n",
    "    trainedModel[\"S\", \"bigram\", \"probability\"] = probBigramS\n",
    "    probTrigramS = calculateTrigramProbabilityDistribution(spanish_train, 3, frqBigramS, frqUnigramS)\n",
    "    trainedModel[\"S\", \"trigram\", \"probability\"] = probTrigramS\n",
    "    return trainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the accuracy  of two language model using trained model trainedModel\n",
    "def acuracyCalculation (trainedModel, wordList, lang1, lang2, nGram, testSet):\n",
    "    TP = 0\n",
    "    FN = 0\n",
    "    for word in wordList:\n",
    "        if calculateProbabilityOfTesData(word.lower(), nGram, trainedModel, lang1) >= calculateProbabilityOfTesData(word.lower(), nGram, trainedModel, lang2):\n",
    "            TP += 1  \n",
    "        else:\n",
    "            FN += 1    \n",
    "    Accuracy = TP / (TP + FN)\n",
    "    print(\"Accuracy of {} vs {} using {}-gram model with {} testSet is {}%\".format(lang1, lang2, nGram, testSet, Accuracy*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 01 : (Language Model Creation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of E vs F using 1-gram model with English testSet is 78.9%\n",
      "Accuracy of E vs F using 2-gram model with English testSet is 81.89999999999999%\n",
      "Accuracy of E vs F using 3-gram model with English testSet is 86.7%\n",
      "Accuracy of F vs E using 1-gram model with Frernch testSet is 71.8%\n",
      "Accuracy of F vs E using 2-gram model with French testSet is 78.5%\n",
      "Accuracy of F vs E using 3-gram model with French testSet is 85.2%\n"
     ]
    }
   ],
   "source": [
    "trainedModel = generateAndStoreTrainedLanguageModel()\n",
    "acuracyCalculation(trainedModel, english_test, \"E\", \"F\", 1, \"English\")\n",
    "acuracyCalculation(trainedModel, english_test, \"E\", \"F\", 2, \"English\")\n",
    "acuracyCalculation(trainedModel, english_test, \"E\", \"F\", 3, \"English\")\n",
    "\n",
    "acuracyCalculation(trainedModel, french_test, \"F\", \"E\", 1, \"Frernch\")\n",
    "acuracyCalculation(trainedModel, french_test, \"F\", \"E\", 2, \"French\")\n",
    "acuracyCalculation(trainedModel, french_test, \"F\", \"E\", 3, \"French\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 02: (Language Model Comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of S vs I using 1-gram model with Spanish testSet is 71.0%\n",
      "Accuracy of S vs I using 2-gram model with Spanish testSet is 85.1%\n",
      "Accuracy of S vs I using 3-gram model with Spanish testSet is 85.6%\n",
      "Accuracy of I vs S using 1-gram model with Italian testSet is 58.5%\n",
      "Accuracy of I vs S using 2-gram model with Italian testSet is 75.5%\n",
      "Accuracy of I vs S using 3-gram model with Italian testSet is 83.8%\n"
     ]
    }
   ],
   "source": [
    "acuracyCalculation(trainedModel, spanish_test, \"S\", \"I\", 1, \"Spanish\")\n",
    "acuracyCalculation(trainedModel, spanish_test, \"S\", \"I\", 2, \"Spanish\")\n",
    "acuracyCalculation(trainedModel, spanish_test, \"S\", \"I\", 3, \"Spanish\")\n",
    "\n",
    "acuracyCalculation(trainedModel, italian_test, \"I\", \"S\", 1, \"Italian\")\n",
    "acuracyCalculation(trainedModel, italian_test, \"I\", \"S\", 2, \"Italian\")\n",
    "acuracyCalculation(trainedModel, italian_test, \"I\", \"S\", 3, \"Italian\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since we get the lowest accuracey for the pair Italian vs Spanish when it is tested with Spanish test data  in 1-gram accuracy 71% and tested with Italian test data 1-gram accuracy 58.5%, this pair is harder to distinguish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
